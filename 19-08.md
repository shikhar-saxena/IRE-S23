---
title: Class 6
date: August 19, 2023
---

||Offline evaluation|Online evaluation|
|-|-|-|
|Basic Assumption|Assessors tell you what is relevant|Observable user behaviour that can tell you what's relevant|

# Null Hypothesis

**Statistically significant**: Rejection of the Null Hypothesis.

- p-value 

Read about student t-test.

- MAP: Mean Average Precision
    - Average over one query and max over all queries

# Precision & Recall (revisited)

- You retrieve all the Positive ones (FP + TP).
- And all relevant ones are (TP + FN).
- TN also not retrieved.
- Classes here: (+) Retrieved / (-) Not Retrieved
$$
\text{Precision} = P = P(\text{relevant} | \text{retrieved}) = \frac{TP}{TP + FP}
$$

$$
\text{Recall} = R = P(\text{retrieved} | \text{relevant}) = \frac{TP}{TP + FN}
$$ 

$$
\text{Accuracy} = \frac{TP + TN}{N} 
$$ 

- Why don't we use accuracy?
    - This is because accuracy might be very high for even a flawed model. So this metric is not good to measure performance.
- How can we generalize the performance?
    - We use f-measure / F1 score instead.


$$F_\beta = \frac{(\beta^2 + 1) \cdot P \cdot R}{\beta^2 P + R}$$ 

- $\beta$: relative importance of recall and precision (popular value $\beta =1$).
- heavily penalized small values of $P$ and $R$.

